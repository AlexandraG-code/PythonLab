import pandas as pd
import numpy as np
import warnings

warnings.filterwarnings('ignore')

# 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...")
df = pd.read_parquet('dns.parquet')
print(f"‚úÖ –î–∞–Ω–Ω—ã–µ –∑–∞–≥—Ä—É–∂–µ–Ω—ã: {df.shape[0]} —Å—Ç—Ä–æ–∫, {df.shape[1]} —Å—Ç–æ–ª–±—Ü–æ–≤")
print(f"‚úÖ –ú–µ—Ç–∫–∞ –∫–ª–∞—Å—Å–∞: 'GlobalClass' (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ: normal/malicious)")

# 2. –ü–æ—Å–º–æ—Ç—Ä–∏–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤
if 'GlobalClass' in df.columns:
    print("\nüéØ –†–ê–°–ü–†–ï–î–ï–õ–ï–ù–ò–ï –ö–õ–ê–°–°–û–í:")
    class_dist = df['GlobalClass'].value_counts()
    for class_name, count in class_dist.items():
        percentage = (count / len(df)) * 100
        print(f"   {class_name}: {count:,} –∑–∞–ø–∏—Å–µ–π ({percentage:.1f}%)")

# 3. –ê–Ω–∞–ª–∏–∑ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
print("\n" + "=" * 60)
print("üîé –ü–û–ò–°–ö –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–• –ü–ê–¢–¢–ï–†–ù–û–í")
print("=" * 60)

# –°–æ–∑–¥–∞–¥–∏–º –∫–æ–ø–∏—é –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
analysis_df = df.copy()

# 3.1. –ê–Ω–æ–º–∞–ª–∏–∏ –≤ —á–∞—Å—Ç–æ—Ç–∞—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
print("\nüìä –ê–ù–û–ú–ê–õ–ò–ò –í –ß–ê–°–¢–û–¢–ê–• –ó–ê–ü–†–û–°–û–í:")

# –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ —á–∞—Å—Ç–æ—Ç—ã (—Å–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–µ/–Ω–∏–∑–∫–∏–µ)
suspicious_freq_features = {
    'NULL_frequency': 'NULL-–∑–∞–ø—Ä–æ—Å—ã (—Ä–µ–¥–∫–∏–µ, –º–æ–≥—É—Ç –±—ã—Ç—å –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–º–∏)',
    'TXT_frequency': 'TXT-–∑–∞–ø—Ä–æ—Å—ã (–∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –¥–ª—è –¥–∞–Ω–Ω—ã—Ö, –≤–æ–∑–º–æ–∂–Ω–∞ —ç–∫—Å—Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è)',
    'OPT_frequency': 'OPT-–∑–∞–ø—Ä–æ—Å—ã (EDNS, –º–æ–≥—É—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å—Å—è –¥–ª—è –∞—Ç–∞–∫)'
}

for feature, description in suspicious_freq_features.items():
    if feature in analysis_df.columns:
        high_threshold = analysis_df[feature].quantile(0.95)  # –≤–µ—Ä—Ö–Ω–∏–µ 5%
        suspicious = analysis_df[analysis_df[feature] > high_threshold]
        if len(suspicious) > 0:
            print(f"   ‚Ä¢ {description}: {len(suspicious):,} –∑–∞–ø–∏—Å–µ–π —Å –≤—ã—Å–æ–∫–æ–π —á–∞—Å—Ç–æ—Ç–æ–π")
            if 'GlobalClass' in analysis_df.columns:
                if 'malicious' in analysis_df['GlobalClass'].values:
                    malicious_in_susp = suspicious[suspicious['GlobalClass'] == 'malicious']
                    print(f"     –°—Ä–µ–¥–∏ –Ω–∏—Ö malicious: {len(malicious_in_susp):,}")

# 3.2. –ê–Ω–æ–º–∞–ª–∏–∏ –≤ —ç–Ω—Ç—Ä–æ–ø–∏–∏
print("\nüîê –ê–ù–û–ú–ê–õ–ò–ò –í –≠–ù–¢–†–û–ü–ò–ò:")
entropy_cols = ['entropy', 'rr_name_entropy']
for col in entropy_cols:
    if col in analysis_df.columns:
        high_entropy = analysis_df[col].quantile(0.95)
        low_entropy = analysis_df[col].quantile(0.05)

        high_susp = analysis_df[analysis_df[col] > high_entropy]
        low_susp = analysis_df[analysis_df[col] < low_entropy]

        print(f"   ‚Ä¢ {col}:")
        print(f"     - –í—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è (> {high_entropy:.2f}): {len(high_susp):,} –∑–∞–ø–∏—Å–µ–π")
        print(f"     - –ù–∏–∑–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è (< {low_entropy:.2f}): {len(low_susp):,} –∑–∞–ø–∏—Å–µ–π")

# 3.3. –ê–Ω–æ–º–∞–ª–∏–∏ –≤ –¥–ª–∏–Ω–µ –∏–º–µ–Ω
print("\nüìè –ê–ù–û–ú–ê–õ–ò–ò –í –î–õ–ò–ù–ï –ò–ú–ï–ù:")
if 'len' in analysis_df.columns or 'rr_name_length' in analysis_df.columns:
    len_col = 'len' if 'len' in analysis_df.columns else 'rr_name_length'

    # –°–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ –∏–º–µ–Ω–∞ (> 95 –ø–µ—Ä—Ü–µ–Ω—Ç–∏–ª—å)
    long_threshold = analysis_df[len_col].quantile(0.95)
    very_long = analysis_df[analysis_df[len_col] > 63]  # RFC –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ
    long_names = analysis_df[analysis_df[len_col] > long_threshold]

    print(f"   ‚Ä¢ –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –∏–º–µ–Ω–∞ (> 63 —Å–∏–º–≤–æ–ª–æ–≤): {len(very_long):,} –∑–∞–ø–∏—Å–µ–π")
    print(f"   ‚Ä¢ –î–ª–∏–Ω–Ω—ã–µ –∏–º–µ–Ω–∞ (> {long_threshold:.0f} —Å–∏–º–≤–æ–ª–æ–≤): {len(long_names):,} –∑–∞–ø–∏—Å–µ–π")

# 3.4. –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ TTL –∑–Ω–∞—á–µ–Ω–∏—è
print("\n‚è±Ô∏è  –ê–ù–û–ú–ê–õ–ò–ò –í TTL:")
ttl_cols = ['unique_ttl', 'ttl_mean', 'ttl_variance']
for col in ttl_cols:
    if col in analysis_df.columns:
        # –ù–∏–∑–∫–∏–π TTL –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–º –±—ã—Å—Ç—Ä–æ–≥–æ —Ñ–ª—É–¥–∏–Ω–≥–∞
        if col == 'unique_ttl':
            low_ttl = analysis_df[analysis_df[col] < 10]  # –ú–µ–Ω—å—à–µ 10 —Å–µ–∫—É–Ω–¥
            print(f"   ‚Ä¢ {col} < 10 —Å–µ–∫: {len(low_ttl):,} –∑–∞–ø–∏—Å–µ–π")
        # –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è TTL
        if col == 'ttl_variance':
            high_var = analysis_df[analysis_df[col] > analysis_df[col].quantile(0.95)]
            print(f"   ‚Ä¢ –í—ã—Å–æ–∫–∞—è –≤–∞—Ä–∏–∞—Ü–∏—è TTL: {len(high_var):,} –∑–∞–ø–∏—Å–µ–π")

# 3.5. –ì–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏–µ –∞–Ω–æ–º–∞–ª–∏–∏
print("\nüåç –ì–ï–û–ì–†–ê–§–ò–ß–ï–°–ö–ò–ï –ê–ù–û–ú–ê–õ–ò–ò:")
geo_cols = ['unique_country', 'unique_asn']
for col in geo_cols:
    if col in analysis_df.columns:
        # –ú–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Å—Ç—Ä–∞–Ω/ASN –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–º DGA
        many_unique = analysis_df[analysis_df[col] > analysis_df[col].quantile(0.95)]
        print(f"   ‚Ä¢ –ú–Ω–æ–≥–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö {col}: {len(many_unique):,} –∑–∞–ø–∏—Å–µ–π")

# 4. –ö–û–ú–ü–û–ó–ò–¢–ù–´–ô –ê–ù–ê–õ–ò–ó: –ü–æ–∏—Å–∫ —Å–∞–º—ã—Ö –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π
print("\n" + "=" * 60)
print("üéØ –í–´–Ø–í–õ–ï–ù–ò–ï –°–ê–ú–´–• –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–• –ó–ê–ü–ò–°–ï–ô")
print("=" * 60)

# –°–æ–∑–¥–∞–¥–∏–º "–±–∞–ª–ª –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"
analysis_df['suspicion_score'] = 0

# –í–µ—Å–∞ –¥–ª—è —Ä–∞–∑–Ω—ã—Ö –∞–Ω–æ–º–∞–ª–∏–π
if 'NULL_frequency' in analysis_df.columns:
    analysis_df['suspicion_score'] += (analysis_df['NULL_frequency'] > analysis_df['NULL_frequency'].quantile(0.9)) * 2

if 'TXT_frequency' in analysis_df.columns:
    analysis_df['suspicion_score'] += (analysis_df['TXT_frequency'] > analysis_df['TXT_frequency'].quantile(0.9)) * 2

if 'entropy' in analysis_df.columns:
    analysis_df['suspicion_score'] += (analysis_df['entropy'] > analysis_df['entropy'].quantile(0.95)) * 3

if 'len' in analysis_df.columns:
    analysis_df['suspicion_score'] += (analysis_df['len'] > 63) * 2

if 'unique_country' in analysis_df.columns:
    analysis_df['suspicion_score'] += (analysis_df['unique_country'] > analysis_df['unique_country'].quantile(0.9)) * 1

# –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è
analysis_df['is_suspicious'] = analysis_df['suspicion_score'] >= 3

# –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
suspicious_count = analysis_df['is_suspicious'].sum()
total_count = len(analysis_df)
suspicious_percent = (suspicious_count / total_count) * 100

print(f"üîç –ù–∞–π–¥–µ–Ω–æ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π: {suspicious_count:,} –∏–∑ {total_count:,} ({suspicious_percent:.1f}%)")

# 5. –°–†–ê–í–ù–ï–ù–ò–ï –° –ò–°–•–û–î–ù–´–ú–ò –ú–ï–¢–ö–ê–ú–ò (–µ—Å–ª–∏ –µ—Å—Ç—å)
if 'GlobalClass' in analysis_df.columns:
    print("\n" + "=" * 60)
    print("üìä –°–†–ê–í–ù–ï–ù–ò–ï –° –ò–°–•–û–î–ù–´–ú–ò –ú–ï–¢–ö–ê–ú–ò")
    print("=" * 60)

    confusion_matrix = pd.crosstab(analysis_df['GlobalClass'], analysis_df['is_suspicious'])
    print("–ú–∞—Ç—Ä–∏—Ü–∞ —Å–æ–ø—Ä—è–∂–µ–Ω–Ω–æ—Å—Ç–∏:")
    print(confusion_matrix)

    # –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –Ω–∞—à–∏—Ö —ç–≤—Ä–∏—Å—Ç–∏–∫
    if 'malicious' in analysis_df['GlobalClass'].values:
        malicious_total = (analysis_df['GlobalClass'] == 'malicious').sum()
        detected_malicious = ((analysis_df['GlobalClass'] == 'malicious') & analysis_df['is_suspicious']).sum()

        detection_rate = (detected_malicious / malicious_total) * 100 if malicious_total > 0 else 0

        print(f"\nüìà –≠–§–§–ï–ö–¢–ò–í–ù–û–°–¢–¨ –û–ë–ù–ê–†–£–ñ–ï–ù–ò–Ø:")
        print(f"   –í—Å–µ–≥–æ malicious: {malicious_total:,}")
        print(f"   –û–±–Ω–∞—Ä—É–∂–µ–Ω–æ –Ω–∞—à–∏–º–∏ –ø—Ä–∞–≤–∏–ª–∞–º–∏: {detected_malicious:,}")
        print(f"   –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è: {detection_rate:.1f}%")

# 6. –¢–û–ü –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–• –ó–ê–ü–ò–°–ï–ô
print("\n" + "=" * 60)
print("üèÜ –¢–û–ü-10 –°–ê–ú–´–• –ü–û–î–û–ó–†–ò–¢–ï–õ–¨–ù–´–• –ó–ê–ü–ò–°–ï–ô")
print("=" * 60)

top_suspicious = analysis_df.sort_values('suspicion_score', ascending=False).head(10)

for i, (idx, row) in enumerate(top_suspicious.iterrows(), 1):
    print(f"\n{i}. [–°—á–µ—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏: {row['suspicion_score']}]")

    if 'rr' in row and pd.notna(row['rr']):
        print(f"   –î–æ–º–µ–Ω: {row['rr']}")

    # –ü—Ä–∏—á–∏–Ω—ã –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    reasons = []
    if 'NULL_frequency' in row and row['NULL_frequency'] > analysis_df['NULL_frequency'].quantile(0.9):
        reasons.append(f"–≤—ã—Å–æ–∫–∞—è NULL —á–∞—Å—Ç–æ—Ç–∞ ({row['NULL_frequency']:.3f})")
    if 'TXT_frequency' in row and row['TXT_frequency'] > analysis_df['TXT_frequency'].quantile(0.9):
        reasons.append(f"–≤—ã—Å–æ–∫–∞—è TXT —á–∞—Å—Ç–æ—Ç–∞ ({row['TXT_frequency']:.3f})")
    if 'entropy' in row and row['entropy'] > analysis_df['entropy'].quantile(0.95):
        reasons.append(f"–≤—ã—Å–æ–∫–∞—è —ç–Ω—Ç—Ä–æ–ø–∏—è ({row['entropy']:.2f})")
    if 'len' in row and row['len'] > 63:
        reasons.append(f"–¥–ª–∏–Ω–∞ {row['len']} —Å–∏–º–≤–æ–ª–æ–≤")

    if reasons:
        print(f"   –ü—Ä–∏—á–∏–Ω—ã: {', '.join(reasons)}")

    if 'GlobalClass' in row:
        print(f"   –ò—Å—Ö–æ–¥–Ω–∞—è –º–µ—Ç–∫–∞: {row['GlobalClass']}")

# 7. –°–û–•–†–ê–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í
print("\nüíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤...")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Å–µ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏
suspicious_df = analysis_df[analysis_df['is_suspicious']].copy()
suspicious_df.to_csv('suspicious_dns_records.csv', index=False, encoding='utf-8-sig')

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å –±–∞–ª–ª–∞–º–∏
analysis_df.to_csv('full_dns_analysis.csv', index=False, encoding='utf-8-sig')

print(f"‚úÖ –ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ –∑–∞–ø–∏—Å–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: suspicious_dns_records.csv ({len(suspicious_df)} –∑–∞–ø–∏—Å–µ–π)")
print(f"‚úÖ –ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: full_dns_analysis.csv")

# 8. –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø
try:
    import matplotlib.pyplot as plt

    print("\nüìà –°–æ–∑–¥–∞–Ω–∏–µ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π...")

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))

    # 1. –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ suspicion_score
    ax1 = axes[0, 0]
    scores = analysis_df['suspicion_score'].value_counts().sort_index()
    ax1.bar(scores.index.astype(str), scores.values)
    ax1.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –±–∞–ª–ª–æ–≤ –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏')
    ax1.set_xlabel('–ë–∞–ª–ª')
    ax1.set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–ø–∏—Å–µ–π')

    # 2. –≠–Ω—Ç—Ä–æ–ø–∏—è —É –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö/–Ω–æ—Ä–º–∞–ª—å–Ω—ã—Ö
    if 'entropy' in analysis_df.columns:
        ax2 = axes[0, 1]
        normal = analysis_df[~analysis_df['is_suspicious']]['entropy'].dropna()
        suspicious = analysis_df[analysis_df['is_suspicious']]['entropy'].dropna()
        ax2.boxplot([normal.values[:1000], suspicious.values[:1000]], labels=['–ù–æ—Ä–º–∞–ª—å–Ω—ã–µ', '–ü–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã–µ'])
        ax2.set_title('–≠–Ω—Ç—Ä–æ–ø–∏—è –¥–æ–º–µ–Ω–Ω—ã—Ö –∏–º–µ–Ω')
        ax2.set_ylabel('–≠–Ω—Ç—Ä–æ–ø–∏—è')

    # 3. –î–ª–∏–Ω–∞ –∏–º–µ–Ω
    if 'len' in analysis_df.columns:
        ax3 = axes[0, 2]
        analysis_df['len'].hist(bins=50, alpha=0.7, ax=ax3)
        ax3.axvline(x=63, color='red', linestyle='--', label='RFC –ª–∏–º–∏—Ç (63)')
        ax3.set_title('–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –¥–ª–∏–Ω—ã –∏–º–µ–Ω')
        ax3.set_xlabel('–î–ª–∏–Ω–∞ —Å–∏–º–≤–æ–ª–æ–≤')
        ax3.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞')
        ax3.legend()

    # 4. NULL frequency
    if 'NULL_frequency' in analysis_df.columns:
        ax4 = axes[1, 0]
        analysis_df['NULL_frequency'].hist(bins=50, alpha=0.7, ax=ax4, log=True)
        ax4.set_title('–ß–∞—Å—Ç–æ—Ç–∞ NULL –∑–∞–ø—Ä–æ—Å–æ–≤ (–ª–æ–≥ —à–∫–∞–ª–∞)')
        ax4.set_xlabel('–ß–∞—Å—Ç–æ—Ç–∞')
        ax4.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞ (–ª–æ–≥)')

    # 5. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
    if 'GlobalClass' in analysis_df.columns:
        ax5 = axes[1, 1]
        if 'malicious' in analysis_df['GlobalClass'].values:
            comparison = analysis_df.groupby('GlobalClass')['is_suspicious'].mean() * 100
            comparison.plot(kind='bar', ax=ax5, color=['green', 'red'])
            ax5.set_title('–ü—Ä–æ—Ü–µ–Ω—Ç –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö –ø–æ –∫–ª–∞—Å—Å–∞–º')
            ax5.set_ylabel('% –ø–æ–¥–æ–∑—Ä–∏—Ç–µ–ª—å–Ω—ã—Ö')
            ax5.set_xticklabels(ax5.get_xticklabels(), rotation=0)

    # 6. TXT frequency
    if 'TXT_frequency' in analysis_df.columns:
        ax6 = axes[1, 2]
        analysis_df['TXT_frequency'].hist(bins=50, alpha=0.7, ax=ax6, log=True)
        ax6.set_title('–ß–∞—Å—Ç–æ—Ç–∞ TXT –∑–∞–ø—Ä–æ—Å–æ–≤ (–ª–æ–≥ —à–∫–∞–ª–∞)')
        ax6.set_xlabel('–ß–∞—Å—Ç–æ—Ç–∞')
        ax6.set_ylabel('–ß–∞—Å—Ç–æ—Ç–∞ (–ª–æ–≥)')

    plt.tight_layout()
    plt.savefig('dns_features_analysis.png', dpi=150, bbox_inches='tight')
    plt.show()
    print("‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: dns_features_analysis.png")

except ImportError:
    print("‚ÑπÔ∏è  –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ matplotlib –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏: pip install matplotlib")

print("\n‚úÖ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")